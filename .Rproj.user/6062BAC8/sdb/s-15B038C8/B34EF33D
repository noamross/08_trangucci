{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Hierarchical Gaussian Processes in Stan\"\nauthor: \"Rob Trangucci\"\noutput:\n  pdf_document: default\n  html_document: default\nbibliography: bib_inf_priors.bib\n---\n\n```{r load_packages, results=\"hide\", message=FALSE, echo=FALSE}\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(rstan)\nlibrary(reshape2)\nlibrary(printr)\nset.seed(123)\nmultiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {\n  library(grid)\n  \n  # Make a list from the ... arguments and plotlist\n  plots <- c(list(...), plotlist)\n  \n  numPlots = length(plots)\n  \n  # If layout is NULL, then use 'cols' to determine layout\n  if (is.null(layout)) {\n    # Make the panel\n    # ncol: Number of columns of plots\n    # nrow: Number of rows needed, calculated from # of cols\n    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),\n                     ncol = cols, nrow = ceiling(numPlots/cols))\n  }\n  \n  if (numPlots==1) {\n    print(plots[[1]])\n    \n  } else {\n    # Set up the page\n    grid.newpage()\n    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))\n    \n    # Make each plot, in the correct location\n    for (i in 1:numPlots) {\n      # Get the i,j matrix positions of the regions that contain this subplot\n      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))\n      \n      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,\n                                      layout.pos.col = matchidx$col))\n    }\n  }\n}\n```\n\n\\section{Introduction}\n\nStan's library has been expanded with functions that facilitate adding Gaussian \nprocesses (GPs) to Stan models. I will share the best practices for coding GPs \nin Stan, and demonstrate how GPs can be added as one component of a larger model.\n\nI would like to thank Aki Vehtari, Michael Betancourt, and the reviewers for\ntheir helpful comments and suggestions.\n\n\\subsection{Gaussian processes regression}\n\nSuppose there are N observations of univariate data, $y$, each associated with\nscalar $x$.\n\nThe generative model for a Gaussian process regression is as follows:\n\n\\begin{align*}\n  \\theta & \\sim g(\\phi) \\\\\n  f(x) & \\sim \\text{GP}\\left( \\mu(x),\n  k_\\theta(x) \\right) \\\\\n  y_i & \\sim \\mathcal{N}\\left( f(x_i), \\sigma \\right) \\forall i\n\\end{align*}\n\nA GP is a stochastic process, indexed by $x \\in \\mathbb{R}$. Any finite sample\nrealized from this stochastic process is jointly multivariate normal\n[@cramer2004stationary]. $\\mu(x)$ is the mean of $f(x)$, and $k_\\theta(x)$, a\nkernel, defines the covariance between any two evaluations of $f(x)$:\n\n\\begin{align*}\n\\text{cov}(f(x_i),f(x_j)) = k(x_i, x_j | \\theta)\n\\end{align*}\n\nThe kernel $k$ is parameterized by a vector $\\theta$, and is required\nto a be a positive-semidefinite function [@rasmussen2005gaussian].\n\nThe finite-dimensional generative model for the GP is:\n\n\\begin{align*}\n\\theta & \\sim g(\\phi) \\\\\nf & \\sim \\text{MultiNormal}(0, K_{\\theta}(x)) \\\\\ny_i & \\sim \\text{Normal}(f_i, \\sigma) \\, \\forall i \\in \\{1,\\dots,N\\}\n\\end{align*}\n\nAbove, $K_{\\theta}(x)$ is an $N \\times N$ covariance matrix, where each entry \n$K[i, j] = k(x_i, x_j | \\theta)$. The exponentiated quadratic kernel has two\ncomponents to theta, $\\alpha$, the marginal standard deviation of the stochastic\nprocess $f$ and $\\ell$, the process length-scale.\n\n\\begin{align}\n  k(x_i, x_j | \\theta) = \\alpha^2 \n\\exp \\left(\n\t- \\dfrac{1}{2\\ell^2} (x_{i} - x_{j})^2\n\\right)\n\\end{align}\n\nThis kernel's defining quality is its smoothness; the function is infinitely \ndifferentiable. This can sometimes be unrealistic for applied work (see \n[@stein2012interpolation]), but it will suffice for the examples that follow, as\nit happens to be the only covariance function that has been implemented in the\nStan library as of January 2017.\n\n\\section{Example: GP with normal outcome}\n\n\\subsection{Latent variable formulation}\n\nThe Stan program for the generative model is defined exactly as the \nfinite-dimensional GP probability model above (of course, choosing priors for\n$\\ell$ and $\\alpha$).\n\n```{r engine='cat', engine.opts = list(file = \"simple_latent_gp.stan\", lang = \"stan\")}\nfunctions {\n  vector gp_pred_rng(real[] x_pred,\n                     vector y_is,\n                     real[] x_is,\n                     real alpha,\n                     real length_scale,\n                     real sigma) {\n    vector[size(x_pred)] f_pred;\n    int N_pred;\n    int N;\n    N_pred = size(x_pred);\n    N = rows(y_is);\n\n    {\n      matrix[N, N] L_Sigma;\n      vector[N] K_div_y_is;\n      matrix[N, N_pred] k_x_is_x_pred;\n      matrix[N, N_pred] v_pred;\n      vector[N_pred] f_pred_mu;\n      matrix[N_pred, N_pred] cov_f_pred;\n      matrix[N_pred, N_pred] nug_pred;\n      matrix[N, N] Sigma;\n      Sigma = cov_exp_quad(x_is, alpha, length_scale);\n      for (n in 1:N)\n        Sigma[n, n] = Sigma[n,n] + square(sigma);\n      L_Sigma = cholesky_decompose(Sigma);\n      K_div_y_is = mdivide_left_tri_low(L_Sigma, y_is);\n      K_div_y_is = mdivide_right_tri_low(K_div_y_is',L_Sigma)';\n      k_x_is_x_pred = cov_exp_quad(x_is, x_pred, alpha, length_scale);\n      f_pred_mu = (k_x_is_x_pred' * K_div_y_is); \n      v_pred = mdivide_left_tri_low(L_Sigma, k_x_is_x_pred);\n      cov_f_pred = cov_exp_quad(x_pred, alpha, length_scale) - v_pred' * v_pred;\n      nug_pred = diag_matrix(rep_vector(1e-12,N_pred));\n\n      f_pred = multi_normal_rng(f_pred_mu, cov_f_pred + nug_pred);\n    }\n    return f_pred;\n  }\n}\ndata {\n  int<lower=1> N;\n  int<lower=1> N_pred;\n  vector[N] y;\n  real x[N];\n  real x_pred[N_pred];\n}\nparameters {\n  real<lower=0> length_scale;\n  real<lower=0> alpha;\n  real<lower=0> sigma;\n  vector[N] eta;\n}\ntransformed parameters {\n  vector[N] f;\n  {\n    matrix[N, N] L;\n    matrix[N, N] K;\n    K = cov_exp_quad(x, alpha, length_scale);\n    for (n in 1:N)\n      K[n, n] = K[n, n] + 1e-12;\n    L = cholesky_decompose(K);\n    f = L * eta;\n  }\n}\nmodel {\n  length_scale ~ gamma(2, 20);\n  alpha ~ normal(0, 1);\n  sigma ~ normal(0, 1);\n  eta ~ normal(0, 1);\n  y ~ normal(f, sigma);\n}\ngenerated quantities {\n  vector[N_pred] f_pred;\n  vector[N_pred] y_pred;\n\n  f_pred = gp_pred_rng(x_pred, y, x, alpha, length_scale, sigma);\n  for (n in 1:N_pred)\n    y_pred[n] = normal_rng(f_pred[n], sigma);\n}\n```\n\nSeveral details of the Stan program are important to note. First is the \ngeneration of the covariance matrix with an exponentiated quadratic kernel. The \nfunction \\texttt{cov\\_exp\\_quad} will generate an $N \\times N$ covariance matrix\nif given a length-$N$ array of either reals or vectors, the signal standard\ndeviation, $\\alpha$, and the length-scale $\\ell$.\n\nThe second detail is the small positive number added to the diagonal. In this \ncase it is 1e-12. Because we're using a positive-definite function $k$ to build \na covariance matrix, the resulting matrix should theoretically be positive \ndefinite. However, because we deal in floating-point numbers, covariance\nmatrices generated by \\texttt{cov\\_exp\\_quad} beyond a small dimension will\ntypically not be numerically positive definite. In that case, we need to force\nthe matrix to be positive definite by adding a small bit of noise to the\ndiagonal called jitter. This will conflict with the parameter $\\sigma$. This is\nOK for our purposes, as the amount of noise we've added is quite small compared\nto the scale of $\\sigma$. In most real-world settings where we have noisy\nobservations from a GP, the scale of the jitter will be small compared to the\nnoise.\n\nThe third detail is that we have Cholesky decomposed the covariance\nmatrix $K$ with \\texttt{cholesky\\_decompose}. In any applied application, the \nfinite dimensional sample from a GP is ultimately a multivariate normal with a \nparameterized covariance matrix. As such, we will need to invert the matrix, or\ndecompose it in some way in order to add a multivariate normal density over $f$\nto our log-posterior density. It turns out that the Cholesky decomposition is\nthe best way to decompose that matrix in Stan right now.\n\nWe could have taken our Cholesky factor $L$ of the covariance matrix $K$ above \nand used the function \\texttt{multi\\_normal\\_cholesky} to add a multivariate \nnormal density over $f$ to the log-posterior. Instead, the fourth detail to note\nabove is that we've multiplied the Cholesky factor of the covariance matrix by a\nvector of univariate normals $eta$ so $f$ is implicitly distributed as a \nmultivariate normal random variable. This is called the non-centered \nparameterization of a multivariate normal. Suppose we have a covariance matrix,\n$\\Sigma$. Because it is a proper covariance matrix, there exists a\nlower-triangular matrix $L$ such that $L \\times L^T = \\Sigma$. We know that:\n\n\\begin{align*}\n\\eta_i & \\sim \\text{Normal}(0, 1) \\, \\forall i \\in \\{1,\\dots,N\\} \\\\\nf & = L \\eta \\\\\nf & \\sim \\text{MultiNormal}(0, \\Sigma) \n\\end{align*}\n\nThe reason to express $f$ as a \\texttt{transformed\\_parameter} is because it \nremoves the prior dependence of the density of $f$ on $\\alpha$ and $\\ell$. When \nthe data are weakly informative about the $f$, this can aid in sampling \nefficiently from the joint posterior. See Betancourt and Girolami's excellent \npaper [@betanhier] for more color on the univariate non-centered\nparameterization. As with any Stan program, we should generate some fake data\nfrom a model with fixed parameters and see whether we can recover our\nparameters.\n\n```{r engine='cat', engine.opts = list(file = \"sim_gp_latent.stan\", lang = \"stan\")}\ndata {\n  int<lower=1> N;\n  real<lower=0> length_scale;\n  real<lower=0> alpha;\n  real<lower=0> sigma;\n}\ntransformed data {\n  vector[N] zeros;\n  zeros = rep_vector(0, N);\n}\nmodel {}\ngenerated quantities {\n  real x[N];\n  vector[N] y;\n  vector[N] f;\n  for (n in 1:N)\n    x[n] = uniform_rng(-2,2);\n  {\n    matrix[N, N] cov;\n    matrix[N, N] L_cov;\n    cov = cov_exp_quad(x, alpha, length_scale);\n    for (n in 1:N)\n      cov[n, n] = cov[n, n] + 1e-12;\n    L_cov = cholesky_decompose(cov);\n    f = multi_normal_cholesky_rng(zeros, L_cov);\n  }\n  for (n in 1:N)\n    y[n] = normal_rng(f[n], sigma);\n}\n```\n\n```{r, results=\"hide\", message=FALSE, echo=FALSE, cache=TRUE}\nsim_data_model <- stan_model('stan_models/gp_simple_latent.stan')\n```\n\n```{r, cache=TRUE}\ndat_list <- list(N = 2000, alpha = 1, length_scale = 0.15, sigma = sqrt(0.1))\nset <- sample(1:dat_list$N,size = 30, replace = F)\ndraw <- sampling(sim_data_model,iter=1,algorithm='Fixed_param', chains = 1, data = dat_list,\n                 seed = 363360090)\nsamps <- rstan::extract(draw)\nplt_df = with(samps,data.frame(x = x[1,], y = y[1,], f = f[1,]))\n```\n\nHere's the data:\n\n```{r}\nggplot(data = plt_df[set,], aes(x=x, y=y)) + \n  geom_point(aes(colour = 'Realized data')) + \n  geom_line(data = plt_df, aes(x = x, y = f, colour = 'Latent mean function')) +\n   theme_bw() + theme(legend.position=\"bottom\") +\n  scale_color_manual(name = '', values = c('Realized data'='black','Latent mean function'='red')) +\n  xlab('X') + \n  ylab('y') +\n  ggtitle(paste0('N=',length(set),' from length-scale = 0.15, alpha = 1, sigma = 0.32'))\n```\n\nWe prep the data for modeling and inference in Stan:\n\n```{r}\nstan_data <- list(N = length(set), N_pred = dat_list$N - length(set),\n                  zeros =rep(0,length(set)), x = samps$x[1,set], y = samps$y[1,set],\n                  x_pred = samps$x[1,-set], f_pred = samps$f[1,-set])\n```\n\nWe'll run 4 chains for 2000 iterations each, with \\texttt{adapt\\_delta} set to 0.95.\n\n```{r, results=\"hide\", message=FALSE, echo=FALSE}\ncomp_gp_mod_lat <- stan_model('simple_latent_gp.stan')\n```\n\n```{r, cache=TRUE}\ngp_mod_lat <- sampling(comp_gp_mod_lat, data = stan_data, cores = 4, chains = 4, iter = 2000, control = list(adapt_delta = 0.95))\nsamps_gp_mod_lat <- extract(gp_mod_lat)\npost_pred <- data.frame(x = stan_data$x_pred,\n                        pred_mu = colMeans(samps_gp_mod_lat$f_pred))\nplt_df_rt = data.frame(x = stan_data$x_pred, f = t(samps_gp_mod_lat$f_pred))\nplt_df_rt_melt = melt(plt_df_rt,id.vars = 'x')\np <- ggplot(data = plt_df[set,], aes(x=x, y=y)) + \n  geom_line(data = plt_df_rt_melt, aes(x = x, y = value, group = variable, colour = 'Posterior mean functions'), alpha = 0.15) + theme_bw() + theme(legend.position=\"bottom\") +\n  geom_point(aes(colour = 'Realized data')) + \n  geom_line(data = plt_df, aes(x = x, y = f, colour = 'Latent mean function')) +\n  geom_line(data = post_pred, aes(x = x, y = pred_mu, colour = 'Posterior mean function')) +\n   theme_bw() + theme(legend.position=\"bottom\") +\n  scale_color_manual(name = '', values = c('Realized data'='black','Latent mean function'='red','Posterior mean functions'= 'blue','Posterior mean function'='green')) +\n  xlab('X') + \n  ylab('y') +\n  ggtitle(paste0('N=',length(set),' from length-scale = 0.15, alpha = 1, sigma = 0.32'))\np\n```\n\nHow does the model do at capturing out-of-sample data? One way to examine the model's\nuse in quantifying the uncertainty in predicting out-of-sample data is to measure how many\nout-of-sample data points fall into a certain posterior predictive interval. To that end,\nlet's quantify the 50\\% and the 95\\% posterior predictive intervals generated by our model\nin the generated quantities block, \\texttt{y\\_pred}. \n\n```{r}\nppc_interval_df <- function(yrep, y) {\n  q_95 <- apply(yrep,2,quantile,0.95)\n  q_75 <- apply(yrep,2,quantile,0.75)\n  q_50 <- apply(yrep,2,median)\n  q_25 <- apply(yrep,2,quantile,0.25)\n  q_05 <- apply(yrep,2,quantile,0.05)\n  mu <- colMeans(yrep)\n  df_post_pred <- data.frame(y_obs = y,\n                             q_95 = q_95,\n                             q_75 = q_75,\n                             q_50 = q_50,\n                             q_25 = q_25,\n                             q_05 = q_05,\n                             mu = mu)\n  return(df_post_pred)\n}\nppc_interval_norm_df <- function(means, sds, y) {\n  q_95 <- qnorm(0.95,mean = means, sd = sds)\n  q_75 <- qnorm(0.75,mean = means, sd = sds)\n  q_50 <- qnorm(0.5,mean = means, sd = sds)\n  q_25 <- qnorm(0.25,mean = means, sd = sds)\n  q_05 <- qnorm(0.05,mean = means, sd = sds)\n  df_post_pred <- data.frame(y_obs = y,\n                             q_95 = q_95,\n                             q_75 = q_75,\n                             q_50 = q_50,\n                             q_25 = q_25,\n                             q_05 = q_05,\n                             mu = means)\n  return(df_post_pred)\n}\ninterval_cover <- function(upper, lower, elements) {\n  return(mean(upper >= elements & lower <= elements))\n}\n```\n\n```{r}\nppc_full_bayes <- ppc_interval_df(samps_gp_mod_lat$y_pred, samps$y[1,-set])\n```\n\n`r round(100*interval_cover(ppc_full_bayes$q_95,ppc_full_bayes$q_05, ppc_full_bayes$y_obs))`\\% \nof out-of-sample data points are in the 90\\% posterior predictive interval.\n\n`r round(100*interval_cover(ppc_full_bayes$q_75,ppc_full_bayes$q_25, ppc_full_bayes$y_obs))`\\%\nof out-of-sample data points are in the central 50\\% posterior predictive interval.\n\nWe'll also check the posterior samples for our unknown hyperparameters, $\\ell$, $\\sigma$, and $\\alpha$.\n\n```{r, message=FALSE}\ndf1 <- data.frame(x = samps_gp_mod_lat$alpha)\np1 <- ggplot(data = df1, \n             aes(x = x)) + geom_histogram() + theme_bw() +\n  labs(title = 'Posterior draws of alpha') +\n       xlab('Alpha') + geom_vline(xintercept = dat_list$alpha,colour = 'red')\ndf2 <- data.frame(x = samps_gp_mod_lat$length_scale)\np2 <- ggplot(data = df2, \n             aes(x = x)) + geom_histogram() + theme_bw() +\n  labs(title = 'Posterior draws of length-scale') +\n       xlab('Length-scale') + geom_vline(xintercept = dat_list$length_scale,colour = 'red')\n\ndf3 <- data.frame(x = samps_gp_mod_lat$sigma)\np3 <- ggplot(data = df3, \n             aes(x = x)) + geom_histogram() + theme_bw() +\n  labs(title = 'Posterior draws of sigma') + \n        geom_vline(xintercept = dat_list$sigma,colour = 'red') + xlab('Sigma')\n\nmultiplot(p1, p2, p3, cols = 3)\n```\n\nAll three of the known parameters lie in areas of large posterior mass.\n\n\\subsection{To marginalize or to maximize?}\n\nWe've taken it for granted that we want to marginalize over the hyperparameters \nrather than using a frequentist solution like is so often advocated in the \nliterature (often due to computational constraints!). Let's examine this \ndecision more closely. First, I'll take a quick diversion to learn how we can \nreparameterize the GP with a normal outcome that will make the model more \ncomputationally efficient and make the model more amenable to penalized maximum\nlikelihood estimation of the hyperparameters.\n\n\\subsection{Marginal likelihood formulation}\n\nWe can also express the GP with a normal likelihood like so:\n\n\\begin{align}\n  p(y | \\alpha, \\ell, \\sigma) = \\int p(y | f, \\sigma) p(f | \\alpha, \\ell) df\n\\end{align}\n\nIt turns out that $p(y | \\alpha, \\ell, \\sigma)$ is multivariate normal with with\na covariance matrix parameterized by a new kernel that integrates the noise\n$\\sigma$ with the exponentiated quadratic:\n\n\\begin{align}\n  k(x_i, x_j | \\theta) = \\alpha^2 \n\\exp \\left(\n\t- \\dfrac{1}{2\\ell^2} (x_{i} - x_{j})^2\n\\right) + \\delta_{ij} \\sigma^2\n\\end{align}\n\nAbove, $\\delta_{ij}$ is the Dirac delta function, taking the value of 1 when $i = j$ and\nremaining zero otherwise.\n\nThus, the generative model is now:\n\n\\begin{align}\n\\ell & \\sim \\text{Gamma}(2,2) \\\\\n\\alpha & \\sim \\text{Half-Normal}(0, 1) \\\\\n\\sigma & \\sim \\text{Half-Normal}(0, 1) \\\\\n\\mathbf{y} & \\sim \\text{MultiNormal}(0, K_{\\ell,\\alpha}(\\mathbf{x},\\mathbf{x}) + \\sigma^2 I_n) \\\\\n&            \\mathbf{y}, \\mathbf{x} \\in \\mathbb{R}^N\n\\end{align}\n\nThis is a much lower-dimensional representation of a GP. It takes the model from a\n$N + 3$ dimensional parameter space to a 3-dimensional parameter space. This will\nreduce the memory requirements substantially for our inference on the hyperparameters, as \nwell as decreasing the dimension of the gradients.\n\nWe code that in Stan as follows, omitting the function block and the generated \nquantities block:\n\n\n```{r engine='cat', engine.opts = list(file = \"simple_marginal_gp.stan\", lang = \"stan\")}\ndata {\n  int<lower=1> N;\n  int<lower=1> N_pred;\n  vector[N] y;\n  real x[N];\n  real x_pred[N_pred];\n}\ntransformed data {\n  vector[N] zeros;\n  \n  zeros = rep_vector(0, N);\n} \nparameters {\n  real<lower=0> length_scale;\n  real<lower=0> alpha;\n  real<lower=0> sigma;\n}\nmodel {\n  matrix[N, N] L_cov;\n  {\n    matrix[N, N] cov;\n    cov = cov_exp_quad(x, alpha, length_scale);\n    for (n in 1:N)\n      cov[n, n] = cov[n, n] + square(sigma);\n    L_cov = cholesky_decompose(cov);\n  }\n//  length_scale ~ gamma(2, 20);\n//  alpha ~ normal(0, 1);\n//  sigma ~ normal(0, 1);\n  y ~ multi_normal_cholesky(zeros, L_cov);\n}\n```\n\nLet's find the penalized MLEs for $\\alpha$, $\\delta$, and $\\ell$. We can do this\neasily in RStan by calling \\texttt{optimizing} on the compiled model.\n\n```{r, results=\"hide\", message=FALSE, echo=FALSE, cache=TRUE}\nmarg_model <- stan_model('simple_marginal_gp.stan')\n```\n\n```{r, results=\"hide\", message=FALSE, cache=TRUE}\nmarg_model_opt <- optimizing(marg_model, data = stan_data)\n```\n\n```{r}\nmarg_model_opt$par\n```\n\nWe can see that these estimates are biased a bit, but this is to be expected \nbecause we have a finite sample of data and priors on the hyperparameters.\n\n```{r}\nstan_data_marg <- stan_data\nstan_data_marg$length_scale = marg_model_opt$par['length_scale']\nstan_data_marg$alpha = marg_model_opt$par['alpha']\nstan_data_marg$sigma = marg_model_opt$par['sigma']\n```\n\n\n```{r engine='cat', engine.opts = list(file = \"simple_marginal_gp_sim.stan\", lang = \"stan\")}\nfunctions {\n  matrix gp_pred_rng(real[] x_pred,\n                     vector y_is,\n                     real[] x_is,\n                     real alpha,\n                     real length_scale,\n                     real sigma) {\n    matrix[2,size(x_pred)] f_pred;\n    int N_pred;\n    int N;\n    N_pred = size(x_pred);\n    N = rows(y_is);\n\n    {\n      matrix[N, N] L_Sigma;\n      vector[N] K_div_y_is;\n      matrix[N, N_pred] k_x_is_x_pred;\n      matrix[N, N_pred] v_pred;\n      vector[N_pred] f_pred_mu;\n      matrix[N_pred, N_pred] cov_f_pred;\n      matrix[N_pred, N_pred] nug_pred;\n      matrix[N, N] Sigma;\n      Sigma = cov_exp_quad(x_is, alpha, length_scale);\n      for (n in 1:N)\n        Sigma[n, n] = Sigma[n,n] + square(sigma);\n      L_Sigma = cholesky_decompose(Sigma);\n      K_div_y_is = mdivide_left_tri_low(L_Sigma, y_is);\n      K_div_y_is = mdivide_right_tri_low(K_div_y_is',L_Sigma)';\n      k_x_is_x_pred = cov_exp_quad(x_is, x_pred, alpha, length_scale);\n      f_pred_mu = (k_x_is_x_pred' * K_div_y_is); \n      v_pred = mdivide_left_tri_low(L_Sigma, k_x_is_x_pred);\n      cov_f_pred = cov_exp_quad(x_pred, alpha, length_scale) - v_pred' * v_pred;\n      \n      f_pred[1,] = f_pred_mu';\n      for (n in 1:N_pred)\n        f_pred[2,n] = sqrt(cov_f_pred[n,n] + square(sigma));\n    }\n    return f_pred;\n  }\n}\ndata {\n  int<lower=1> N;\n  int<lower=1> N_pred;\n  vector[N] y;\n  real x[N];\n  real x_pred[N_pred];\n  real<lower=0> sigma;\n  real<lower=0> length_scale;\n  real<lower=0> alpha;\n}\nmodel {\n}\ngenerated quantities {\n  matrix[2,N_pred] f_pred;\n\n  f_pred = gp_pred_rng(x_pred, y, x, alpha, length_scale, sigma);\n}\n```\n\n```{r, results=\"hide\", message=FALSE, echo=FALSE, cache=TRUE}\nmarg_model_sim <- stan_model('simple_marginal_gp_sim.stan')\nmarg_draws <- sampling(marg_model_sim,iter=1,algorithm='Fixed_param',\n                 chains = 1,data = stan_data_marg)\n```\n\n```{r, results =\"hide\", message=FALSE}\nsamps_marg <- rstan::extract(marg_draws)\nppc_max_marg <- ppc_interval_norm_df(samps_marg$f_pred[1,1,], samps_marg$f_pred[1,2,], samps$y[1,-set])\n```\n\n`r round(100*interval_cover(ppc_max_marg$q_95,ppc_max_marg$q_05, ppc_max_marg$y_obs))`\\%\nof out-of-sample data points are in the 90\\% posterior predictive interval.\n\n`r round(100*interval_cover(ppc_max_marg$q_75,ppc_max_marg$q_25, ppc_max_marg$y_obs))`\\% \nof out-of-sample data points are in the 50\\% posterior predictive interval.\n\n```{r, cache=TRUE}\nppc_max_marg$x <- samps$x[1,-set]\nggplot(data = ppc_max_marg, aes(x = x, y = y_obs)) +\n  geom_ribbon(aes(ymax = q_95, ymin = q_05,alpha=0.5, colour = '95% predictive interval')) + geom_point(aes(colour = 'Out-of-sample data'),alpha=0.5) + theme_bw() +\n  geom_point(data = plt_df[set,], aes(x = x, y = y, colour='Observed data')) + theme(legend.position=\"bottom\") +\n  geom_line(data = ppc_max_marg, aes(x = x, y = mu, colour = 'Posterior predictive mean')) +\n  scale_color_manual(name = '', values = c('Observed data'='red',\n                                           '95% predictive interval'='blue',\n                                           'Out-of-sample data'= 'black',\n                                           'Posterior predictive mean'='green')) +\n  xlab('X') + \n  ylab('y') +\n  ggtitle(paste0('MML PP intervals for N=',length(set),' from length-scale = 0.15, alpha = 1, sigma = 0.32'))\n```\n\n```{r, cache=TRUE}\nppc_full_bayes$x <- samps$x[1,-set]\nggplot(data = ppc_full_bayes, aes(x = x, y = y_obs)) +\n  geom_ribbon(aes(ymax = q_95, ymin = q_05,alpha=0.5, colour = '95% predictive interval')) + geom_point(aes(colour = 'Out-of-sample data'),alpha=0.5) + theme_bw() +\n  geom_point(data = plt_df[set,], aes(x = x, y = y, colour='Observed data')) + theme(legend.position=\"bottom\") +\n  geom_line(data = ppc_full_bayes, aes(x = x, y = mu, colour = 'Posterior predictive mean')) +\n  scale_color_manual(name = '', values = c('Observed data'='red',\n                                           '95% predictive interval'='blue',\n                                           'Out-of-sample data'= 'black',\n                                           'Posterior predictive mean'='green')) +\n  xlab('X') + \n  ylab('y') +\n  ggtitle(paste0('Full Bayes PP intervals for N=',length(set),' from length-scale = 0.15, alpha = 1, sigma = 0.32'))\n```\n  \n\\section{GP with Poisson outcome}\n\nLet's say we have count data now, and we want to fit the data using a GP \nprior for the latent mean. \n\nThe generative model for the data will be nearly identical to our normal latent\nvariable model:\n\n\\begin{align*}\n\\theta & \\sim g(\\phi) \\\\\nf & \\sim \\text{MultiNormal}(0, K_{\\theta}(x)) \\\\\ny_i & \\sim \\text{Poisson}(\\exp(f_i)) \\, \\forall i \\in \\{1,\\dots,N\\}\n\\end{align*}\n\nLuckily, we can reuse much of our code from the normal latent variable model\nwhile removing the $\\sigma$ parameter and changing our likelihood to a \nPoisson with a $\\log$ link.\n\n\n```{r engine='cat', engine.opts = list(file = \"pois_latent_gp.stan\", lang = \"stan\")}\ndata {\n  int<lower=1> N;\n  int<lower=1> N_pred;\n  int y[N];\n  real x[N];\n  real x_pred[N_pred];\n}\ntransformed data {\n  int<lower=1> N_tot;\n  int<lower=1> k;\n  real x_tot[N_pred + N];\n  \n  N_tot = N_pred + N;\n  k = 1;\n  for (n in 1:N) {\n    x_tot[k] = x[n];\n    k = k + 1;\n  }\n  for (n in 1:N_pred) {\n    x_tot[k] = x_pred[n];\n    k = k + 1;\n  }\n}\nparameters {\n  real<lower=0> length_scale;\n  real<lower=0> alpha;\n  vector[N_tot] eta;\n}\ntransformed parameters {\n  vector[N_tot] f;\n  {\n    matrix[N_tot, N_tot] L;\n    matrix[N_tot, N_tot] K;\n    K = cov_exp_quad(x_tot, alpha, length_scale);\n    for (n in 1:N_tot)\n      K[n, n] = K[n, n] + 1e-12;\n    L = cholesky_decompose(K);\n    f = L * eta;\n  }\n}\nmodel {\n  length_scale ~ gamma(2, 20);\n  alpha ~ normal(0, 1);\n  eta ~ normal(0, 1);\n  y ~ poisson_log(f[1:N]);\n}\n```\n\nWe can repurpose the fake data from above, by exponentiating the $f$ and generating Poisson\nrandom variables conditioned on $\\exp(f)$:\n\n```{r, cache=TRUE}\npois_oos_set <- sample((1:dat_list$N)[-set],size = 150, replace = F)\npois_N_oos <- length(pois_oos_set)\nN_set <- length(set)\npois_full_set <- c(set,pois_oos_set)\npois_N_full_set <- pois_N_oos + N_set\nstan_data_pois <- list(N = length(set), N_pred = pois_N_oos,\n                       f_all = exp(samps$f[1,]),\n                       x = samps$x[1,set], x_all = samps$x[1,],\n                       y_all = rpois(n = dat_list$N,\n                                     lambda = exp(samps$f[1,])),\n                       x_pred = samps$x[1,pois_oos_set])\nstan_data_pois$y <- stan_data_pois$y_all[set]\n```\n\n```{r, results=\"hide\", message=FALSE, echo=FALSE, cache=TRUE}\ngp_mod_lat_pois <- stan_model('pois_latent_gp.stan')\n```\n\n```{r, message=FALSE, cache=TRUE}\nmod_run_lat_pois <- sampling(gp_mod_lat_pois, data = stan_data_pois, cores = 2, chains = 4, iter = 1000)\nsamps_lat_pois <- rstan::extract(mod_run_lat_pois)\ndf1 <- data.frame(x = samps_lat_pois$alpha)\np1 <- ggplot(data = df1, \n             aes(x = x)) + geom_histogram() + theme_bw() +\n  labs(title = 'Posterior draws of alpha') +\n       xlab('Alpha') + geom_vline(xintercept = dat_list$alpha,colour = 'red')\ndf2 <- data.frame(x = samps_lat_pois$length_scale)\np2 <- ggplot(data = df2, \n             aes(x = x)) + geom_histogram() + theme_bw() +\n  labs(title = 'Posterior draws of length-scale') +\n       xlab('Length-scale') + geom_vline(xintercept = dat_list$length_scale,colour = 'red')\nmultiplot(p1, p2, cols = 2)\n```\n\n```{r, cache=TRUE}\nplt_df = with(stan_data_pois,data.frame(x = c(x_all[set],x_all[pois_oos_set]), \n                                        y = c(y_all[set],y_all[pois_oos_set]),\n                                        f = c(f_all[set],f_all[pois_oos_set])))\nplt_df_rt = data.frame(x = plt_df$x, f = exp(t(samps_lat_pois$f)))\nplt_df_rt_melt = melt(plt_df_rt,id.vars = 'x')\np <- ggplot(data = plt_df[1:length(set),], aes(x=x, y=y)) + \n  geom_point(aes(colour = 'Realized data')) + \n  geom_line(data = plt_df_rt_melt, aes(x = x, y = value, group = variable, colour = 'Posterior mean functions'), alpha = 0.05) + theme_bw() + theme(legend.position=\"bottom\") +\n  geom_line(data = plt_df, aes(x = x, y = f, colour = 'Latent mean function')) +\n  scale_color_manual(name = '', values = c('Realized data'='black','Latent mean function'='red', 'Posterior mean functions'='blue')) +\n  xlab('X') + \n  ylab('y') +\n  ggtitle(paste0('N=',length(set),' from length-scale = 0.15, alpha = 1, sigma = 0.32')) +\n  ylim(c(0,50))\np\n```\n\n\\section{GP example}\n\nAndrew hosted an interesting model on his blog recently; the NYTimes had asked \nsome researchers if they would forecast the 2020, 2024 and 2028 state-wide \npresidential votes. Let's dive into the problem, and see how we can use GPs in\nStan to generate forecasts. Much of the code I'll present below was featured on\nAndrew's blog, and the model we'll walk through is a slight variation on what he\nfitted.\n\nLoading the data:\n\n```{r}\npast_votes <- readRDS('data_pres_forecast/pres_vote_historical.RDS') %>%\n  filter(state != 'DC')\n```\n\nWe're going to exclude DC because it's so different from the other states. We're \ngoing to use exchangeable priors to model time-invariant state effects and time-varying\nstate effects, and DC is sufficiently different from the other states that we should\nbuild separate priors for the nation's capital. I won't do that here because I want\nto showcase integrating GPs into a more complex model, but we certainly could do this\nin the interest of generating better and more-detailed forecasts. \n\n```{r}\npast_votes %>% head()\n```\n\nThe observations are counts of the two-party vote in each state for each \npresidential election.\n\n```{r}\ntable(past_votes$state)\n```\n\nWe have only 11 observations per state, which isn't very many. But there is\nextra structure that we can use to partially pool observations together. \n\nThe outcome we care about is the Republican share of the two-party vote by year\nby state. \n\n```{r}\nstate_groups <- list(c(\"ME\",\"NH\",\"VT\",\"MA\",\"RI\",\"CT\"),\n                      c(\"NY\",\"NJ\",\"PA\",\"MD\",\"DE\"),\n                      c(\"OH\",\"MI\",\"IL\",\"WI\",\"MN\"),\n                      c(\"WA\",\"OR\",\"CA\",\"HI\"),\n                      c(\"AZ\",\"CO\",\"NM\",\"NV\"),\n                      c(\"IA\",\"NE\",\"KS\",\"ND\",\"SD\"),\n                      c(\"KY\",\"TN\",\"MO\",\"WV\",\"IN\"),\n                      c(\"VA\",\"OK\",\"FL\",\"TX\",\"NC\"),\n                      c(\"AL\",\"MS\",\"LA\",\"GA\",\"SC\",\"AR\"),\n                      c(\"MT\",\"ID\",\"WY\",\"UT\",\"AK\"))\nregion_names <- c(\"New England\", \"Mid-Atlantic\", \"Midwest\", \"West Coast\",\n                  \"Southwest\",\"Plains\", \"Border South\", \"Outer South\", \"Deep South\",\n                  \"Mountain West\")\nstate_region_map <- mapply(FUN = function(states, region) \n  data.frame(state = states, \n             region = rep(region,length(states)),\n             stringsAsFactors = F),state_groups,region_names,\n  SIMPLIFY = F)\n\nstate_region_map <- bind_rows(state_region_map) %>%\n  arrange(state) %>% mutate(\n    region_ind = as.integer(as.factor(region))\n  )\n```\n\nWe have 10 regions, with about 5 states per region. In order to get the data\ninto the right form for Stan, we need a list of integers that map each\nobservation to a state, and a separate vector for regions. \n\nJoining all the data together will allow us to plot everything, which will\nelucidate the structure of the data.\n\n```{r}\nyear_map <- data.frame(year = sort(unique(past_votes$year)),\n                       year_ind = 1:11)\npast_votes <- past_votes %>%\n  arrange(state, year) %>%\n  left_join(state_region_map, by = 'state') %>%\n  left_join(year_map, by = 'year') %>%\n  mutate(\n    state_ind = as.integer(as.factor(state)),\n    two_party_turnout = dem + rep,\n    y = rep / two_party_turnout\n  )\n```\n\nHere are the state and region indices matched to each observation:\n\n```{r}\nhead(past_votes[,c('year','state','state_ind','region','region_ind','y')])\n```\n\n```{r}\ntail(past_votes[,c('year','state','state_ind','region','region_ind','y')])\n```\n\nWe're going to plot the time series of the Republican share of the two-party vote\nin each state for the past 11 presidential elections.\n\n```{r}\npast_votes %>%\n  ggplot(aes(x = year, y = y, colour = state)) +\n  geom_line() + facet_wrap(~ region) +\n  theme_bw() + theme(legend.position = 'None') +\n  ylab('Republican share of two-party vote') + xlab('Year')\n```\n\nWe can patterns that we might want to include in a model. Most notably, and \nperhaps not surprisingly, we see that there is a cross-sectional national \ncorrelation each year. There look to be time-invariant state-level mean \nRepublican shares. There should likely be a time-invariant regional offset.\n\nWithin regions, there is also a clear time trend. Some states don't strictly \nadhere to the regional trend. There is a longer-term trend, and then short-term \ndeviations away from the trend. This suggests a model structure that accounts \nfor time-varying and time-invariant state and regional factors, as well as \nnational trends. Because our outcome data will be a proportion we'll use a beta \ndensity for our likelihood. The beta distribution has support in $[0,1]$ and is\ncanonically parameterized with two shape parameters, $\\gamma$ and $\\beta$.\n\n\\begin{align}\np(y | \\gamma, \\beta) = \\dfrac{1}{\\text{B}(\\gamma, \\beta)} \ny ^ {\\gamma - 1} (1 - y)^{\\beta - 1}\n\\end{align}\n\nWe can reparameterize the distribution in terms of its mean $\\mathbb{E}[y] = \\mu$ and precision, $\\text{Var}[y] = \\dfrac{\\mu (1 - \\mu)}{(1 + \\nu)}$\n[@betareg].\n\n\\begin{align}\np(y | \\mu, \\nu) = \\dfrac{1}{\\text{B}(\\mu \\nu, (1 - \\mu) \\nu)} \ny ^ {\\mu \\nu - 1} (1 - y)^{(1 - \\mu) \\nu - 1}\n\\end{align}\n\nWe'll use the alternative parameterization for our regression. Note that \n$\\nu = \\gamma + \\beta$. We can think of $\\nu$ as being the prior sample size,\nlike when using a beta distribution as a conjugate prior for the probability\nparameter in a binomial likelihood. The interpretation of $\\nu$ as sample size\nhelps us to formulate an informative prior for $\\nu$. We use a Gamma distribution\nwith a mean of 500, $\\nu \\sim \\text{Gamma}(5,\\tfrac{1}{100})$ because we have about \n500 observations.\n\n\\begin{align*}\ny_{t,j} & \\sim \\text{Beta}(\\text{inv\\_logit} \\, \\mu_{t,j}, \\nu) \\\\\n\\mu_{t,j} & = \\theta_t^{\\text{year}} \n            + \\theta_j^{\\text{state}}\n            + \\theta_{k[j]} ^{\\text{region}} \\\\\n           & + \\gamma_{t,j} + \\delta_{t,k[j]} \\\\\n\\boldsymbol{\\gamma_{j}} & \\sim \\text{MultiNormal}(0, K_{\\ell^\\gamma_1, \\alpha^\\gamma_1} + K_{\\ell^\\gamma_2, \\alpha^\\gamma_2}) \\\\\n\\boldsymbol{\\delta_{k}} & \\sim \\text{MultiNormal}(0, K_{\\ell^\\delta_1, \\alpha^\\delta_1} + K_{\\ell^\\delta_2, \\alpha^\\delta_2})\n\\end{align*}\n\nIn order to properly identify the model, we'll need strong priors over all of\nthe parameters, because we don't have very much data to work with. A quick way\nto formulate priors that have good shrinkage properties is to put hierarchical \nshrinkage priors on our $\\theta_j^{\\text{state}}$ and\n$\\theta_{k}^{\\text{region}}$:\n\n\\begin{align*}\n\\theta_j^{\\text{state}} & \\sim \\text{Normal}(0, \\sigma^\\text{state}) \\\\\n\\theta_{k}^{\\text{region}} & \\sim \\text{Normal}(0, \\sigma^\\text{region}) \\\\\n\\end{align*}\n\nNote that we can put more structure into the variance parameters for the\nstate-level time-invariant means.\n\n\\begin{align*}\n\\theta_j^{\\text{state}} & \\sim \\text{Normal}(0, \\sigma_{k[j]}^\\text{state}) \\\\\n\\end{align*}\n\nIn order to build forecasts, we'll also need a map of state to region. We have the\nmap from observation to regions from above, but we can use that to build a map\nthat is correctly ordered from state to region:\n\n```{r}\nstan_state_region_map <- unique(past_votes[,c('state_ind','region_ind')]) %>%\n  arrange(state_ind)\n```\n\n```{r}\nstan_state_region_map %>% head()\n```\n\nNow we prep the data for RStan:\n\n```{r}\nto_stan <- with(past_votes,\n                 list(\n                   N = dim(past_votes)[1],\n                   state_region_ind = stan_state_region_map$region_ind,\n                   N_states = length(unique(past_votes$state)),\n                   N_regions = length(unique(past_votes$region)),\n                   N_years_obs = length(unique(past_votes$year)),\n                   state_ind = state_ind,\n                   region_ind = region_ind,\n                   y = y,\n                   year_ind = year_ind,\n                   N_years = 14))\n\n```\n\n\\subsection{Stan program for election forecasting}\n\n```{r engine='cat', engine.opts = list(file = \"hierarchical_gp.stan\", lang = \"stan\")}\ndata {\n  int<lower=1> N;\n  int<lower=1> N_states;\n  int<lower=1> N_regions;\n  int<lower=1> N_years_obs;\n  int<lower=1> N_years;\n  int<lower=1> state_region_ind[N_states];\n  int<lower=1,upper=50> state_ind[N];\n  int<lower=1,upper=10> region_ind[N];\n  int<lower=1> year_ind[N];\n  vector<lower=0,upper=1>[N] y;\n}\ntransformed data {\n  real years[N_years];\n  vector[16] counts;\n  int n_comps;\n\n  for (t in 1:N_years)\n    years[t] = t;\n  n_comps = rows(counts);\n  for (i in 1:n_comps)\n    counts[i] = 2;\n}\nparameters {\n  matrix[N_years,N_regions] GP_region_std;\n  matrix[N_years,N_states] GP_state_std;\n  vector[N_years_obs] year_std;\n  vector[N_states] state_std;\n  vector[N_regions] region_std;\n  real<lower=0> tot_var;\n  simplex[n_comps] prop_var;\n  real mu;\n  real<lower=0> nu;\n\n  real<lower=0> length_GP_region_long;\n  real<lower=0> length_GP_state_long;\n  real<lower=0> length_GP_region_short;\n  real<lower=0> length_GP_state_short;\n}\ntransformed parameters {\n  matrix[N_years,N_regions] GP_region;\n  matrix[N_years,N_states] GP_state;\n\n  vector[N_years_obs] year_re;\n  vector[N_states] state_re;\n  vector[N_regions] region_re;\n  vector[n_comps] vars;\n\n  real sigma_year;\n  real sigma_region;\n  vector[10] sigma_state;\n\n  real sigma_GP_region_long;\n  real sigma_GP_state_long;\n  real sigma_GP_region_short;\n  real sigma_GP_state_short;\n\n  vars = n_comps * prop_var * tot_var;\n  sigma_year = sqrt(vars[1]);\n  sigma_region = sqrt(vars[2]);\n  for (i in 1:10)\n    sigma_state[i] = sqrt(vars[i + 2]);\n\n  sigma_GP_region_long = sqrt(vars[13]);\n  sigma_GP_state_long = sqrt(vars[14]);\n  sigma_GP_region_short = sqrt(vars[15]);\n  sigma_GP_state_short = sqrt(vars[n_comps]);\n\n  region_re = sigma_region * region_std;\n  year_re = sigma_year * year_std;\n  state_re = sigma_state[state_region_ind] .* state_std;\n  \n  {\n    matrix[N_years, N_years] cov_region; \n    matrix[N_years, N_years] cov_state; \n    matrix[N_years, N_years] L_cov_region; \n    matrix[N_years, N_years] L_cov_state; \n\n    cov_region = cov_exp_quad(years, sigma_GP_region_long, \n                                  length_GP_region_long)\n               + cov_exp_quad(years, sigma_GP_region_short, \n                                  length_GP_region_short);\n    cov_state = cov_exp_quad(years, sigma_GP_state_long, \n                                  length_GP_state_long)\n               + cov_exp_quad(years, sigma_GP_state_short, \n                                  length_GP_state_short);\n    for (year in 1:N_years) {\n      cov_region[year, year] = cov_region[year, year] + 1e-12;\n      cov_state[year, year] = cov_state[year, year] + 1e-12;\n    }\n\n    L_cov_region = cholesky_decompose(cov_region);\n    L_cov_state = cholesky_decompose(cov_state);\n    GP_region = L_cov_region * GP_region_std;\n    GP_state = L_cov_state * GP_state_std;\n  }\n}\nmodel {\n  vector[N] obs_mu;\n\n  for (n in 1:N) {\n    obs_mu[n] = nu * inv_logit(mu + year_re[year_ind[n]] \n              + state_re[state_ind[n]] \n              + region_re[region_ind[n]]\n              + GP_region[year_ind[n],region_ind[n]]\n              + GP_state[year_ind[n],state_ind[n]]);\n  }\n  y ~ beta(obs_mu, (nu - obs_mu)); \n\n  to_vector(GP_region_std) ~ normal(0, 1);\n  to_vector(GP_state_std) ~ normal(0, 1);\n  year_std ~ normal(0, 1);\n  state_std ~ normal(0, 1);\n  region_std ~ normal(0, 1);\n  mu ~ normal(0, .5);\n  tot_var ~ gamma(3, 3);\n  nu ~ gamma(5, 0.01);\n  prop_var ~ dirichlet(counts);\n  length_GP_region_long ~ weibull(30,8);\n  length_GP_state_long ~ weibull(30,8);\n  length_GP_region_short ~ weibull(30,3);\n  length_GP_state_short ~ weibull(30,3);\n}\ngenerated quantities {\n  matrix[N_years,N_states] y_new;\n  matrix[N_years,N_states] y_new_pred;\n\n  {\n    real level;\n    level = normal_rng(0.0, sigma_year);\n    for (state in 1:N_states) {\n      for (t in 1:N_years) {\n        if (t < 12) {\n          y_new[t,state] = state_re[state] \n                         + region_re[state_region_ind[state]]\n                         + GP_state[t,state]\n                         + GP_region[t,state_region_ind[state]]\n                         + mu + year_re[t];\n        } else {\n          y_new[t,state] = state_re[state] \n                         + region_re[state_region_ind[state]]\n                         + GP_state[t,state]\n                         + GP_region[t,state_region_ind[state]]\n                         + level;\n        }\n        y_new_pred[t,state] =\n          beta_rng(inv_logit(y_new[t,state]) * nu,\n                   nu * (1 - inv_logit(y_new[t,state])));\n      }\n    }\n  }\n}\n```\n\nOf note in the model above is how we parameterize the hierarchical latent GP model. In \norder to put multivariate normal priors on each $\\boldsymbol{\\gamma_j}$:\n\n\\begin{align*}\n\\boldsymbol{\\gamma_{j}} & \\sim \\text{MultiNormal}(0, K_{\\ell^\\gamma_1, \\alpha^\\gamma_1} + K_{\\ell^\\gamma_2, \\alpha^\\gamma_2})\n\\end{align*}\n\nwe generate an auxiliary set of random variables, $\\eta \\in \\mathbb{R}^{T,J}$, and \ndecompose the covariance matrix $K_{\\ell^\\gamma_1, \\alpha^\\gamma_1} + K_{\\ell^\\gamma_2, \\alpha^\\gamma_2}$ into a lower triangular matrix $L$:\n\n\\begin{align*}\nL \\times L^T = K_{\\ell^\\gamma_1, \\alpha^\\gamma_1} + K_{\\ell^\\gamma_2, \\alpha^\\gamma_2}\n\\end{align*}\n\nLeft multiplying $L$ by $eta$ yields $\\gamma \\in \\mathbb{R}^{T,J}$, where each\ncolumn of $\\gamma$, $\\gamma_j$, is an independent draw from a multivariate normal\ndensity $\\text{MultiNormal}(0, K_{\\ell^\\gamma_1, \\alpha^\\gamma_1} + K_{\\ell^\\gamma_2, \\alpha^\\gamma_2})$.\n\nNow we fit the model. \n```{r, results=\"hide\", message=FALSE, cache=TRUE}\ncompiled_model <- stan_model(\"hierarchical_gp.stan\")\n```\n\n```{r, cache=TRUE}\nfit <- sampling(compiled_model, data = to_stan, iter = 2000,\n                chains = 4, cores = 4, seed = 1245860998)\n```\n\nInspecting the results for convergence of parameters:\n\n```{r}\nsum_fit <- summary(fit)\ntail(sort(sum_fit$summary[,'Rhat']))\nhead(round(100*sort(sum_fit$summary[,'n_eff'])/max(sum_fit$summary[,'n_eff'])))\n```\n\nIt appears that this model converged nicely and the effective number of \nsamples is in the right ballpark for a well-behaved model. What do our\nforecasts look like for the next 3 presidential elections? We have the\ndata we need to answer this question in our generated quantities block.\n\n```{r}\nsamps <- rstan::extract(fit)\nstate_preds_new <- samps$y_new_pred\ndim(state_preds_new)\n```\n\n\nHere's an example of what our forecasts look like for Wyoming. We'd expect\nthis state to be a pretty firm Republican firewall state. \n\n```{r}\nst_map <- unique(past_votes[,c('state','state_ind')])\nst_map %>% filter(state == 'WY') -> wy_ind\n```\n\n```{r}\nyear_vec <- c(sort(unique(past_votes$year)),2020,2024,2028)\nwy_preds <- state_preds_new[,,wy_ind$state_ind]\nwy_df <- data.frame(year = year_vec,\n                    y = t(wy_preds))\n\nwy_melt <- melt(wy_df,id.vars = 'year')\nhead(wy_melt)\nggplot(wy_melt, aes(x = year, y = value, group = variable)) +\n  geom_line(alpha = 0.03) + geom_hline(yintercept = 0.5, colour = 'red') +\n  scale_x_continuous(breaks=year_vec) +\n  theme_bw() +\n  labs(title = 'Posterior draws for Wyoming Republican vote share') +\n       xlab('Year') + \n       ylab('Republican share of two-party vote')\n```\n\nThis comports with our intuition that Wyoming should be firmly Republican in 2020. The \nmodel's posterior mass on a Democratic win in Wyoming in 2020 is less than a percent:\n\n```{r}\ndem_win_wy <- data.frame(year = year_vec, \n                         pct_prob_dem_win = round(100*apply(wy_preds < 0.5,2,mean),2))\ndem_win_wy %>% filter(year > 2016)\n```\n\n```{r}\nl_df <- list()\nfor (i in 1:50) {\n  st_nm <- st_map %>% filter(state_ind == i) \n  st_df <- state_preds_new[,,i]\n  preds <- data.frame(year = year_vec, \n                      y = t(st_df))\n  preds_melt <- melt(preds, id.vars = 'year')\n  preds_melt$state = st_nm$state\n  l_df[[i]] <- preds_melt\n}\nl_df <- bind_rows(l_df)\nl_df <- l_df %>% left_join(state_region_map, by = 'state')\nplts <- list()\nfor (nm_i in seq_along(region_names)) {\n  region_nm <- region_names[nm_i] \n  plts[[region_nm]] <- l_df %>% filter(region == region_nm) %>%\n    ggplot(aes(x = year, y = value, group = variable)) +\n    geom_line(alpha = 0.03) + geom_hline(yintercept = 0.5, colour = 'red') +\n    scale_x_continuous(breaks=year_vec) +\n    theme_bw() + \n    labs(title = paste0('Republican vote share in ',region_nm,' region')) +\n         xlab('Year') + \n         ylab('Republican share of two-party vote') + \n    theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n   facet_wrap(~ state)\n}\n```\n\nHere're the forecasts for New England's states:\n\n```{r}\nplts[['New England']]\n```\n\nHere're the forecasts for the Mid-Atlantic states:\n\n```{r}\nplts[['Mid-Atlantic']]\n```\n\nAnd Midwest states:\n\n```{r}\nplts[['Midwest']]\n```\n\nMaking our way further west, West Coast states:\n\n```{r}\nplts[['West Coast']]\n```\n\nA southerly move:\n\n```{r}\nplts[['Southwest']]\n```\n\nAnd onward to the Plains region:\n\n```{r}\nplts[['Plains']]\n```\n\nSouthern states:\n\n```{r}\nplts[['Border South']]\n```\n\nMore Southern states:\n\n```{r}\nplts[['Outer South']]\n```\n\nThe Deep South:\n\n```{r}\nplts[['Deep South']]\n```\n\nAnd, finally, the Mountain West:\n\n```{r}\nplts[['Mountain West']]\n```\n\n\\section{Conclusion}\n\nGPs are a flexible class of priors over random variables in probabilistic\nmodels. We can integrate Gaussian processes into Stan models easily using \n\\texttt{cov\\_exp\\_quad}. The latent variable formulation of the GP is \nparticularly useful when defining the GP prior over parameters that are\nweakly informed by the data. \n\n# REFERENCES",
    "created" : 1499892224749.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "299205783",
    "id" : "B34EF33D",
    "lastKnownWriteTime" : 1499906370,
    "last_content_update" : 1499892604635,
    "path" : "~/Dropbox (EHA)/projects/08_trangucci/hierarchical_GPs_in_stan.Rmd",
    "project_path" : "hierarchical_GPs_in_stan.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}